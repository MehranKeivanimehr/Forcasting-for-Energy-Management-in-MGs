{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from math import sqrt\n",
    "import os.path\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Dense, LSTM, Flatten, Dropout, Bidirectional, GRU, concatenate, Embedding, Dropout, Input, SpatialDropout1D, Dropout\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, Conv2D\n",
    "\n",
    "optimizer='adam'\n",
    "\n",
    "def plot(tdf):\n",
    "    pyplot.figure()\n",
    "    groups = np.arange(0, tdf.shape[1], 1, dtype=int)\n",
    "    i = 1\n",
    "    values = tdf.values\n",
    "    for group in groups:\n",
    "        pyplot.subplot(len(groups), 1, i)\n",
    "        pyplot.plot(values[:, group])\n",
    "        pyplot.title(tdf.columns[group], y=0.5, loc='right')\n",
    "        i += 1\n",
    "    pyplot.show()\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "def show_result(fmodel, ftest_X, ftest_y, scaler):\n",
    "    \n",
    "    yhat = fmodel.predict(ftest_X)\n",
    "\n",
    "    yhat_real = inverse_data(yhat, scaler.data_max_[-1], scaler.data_min_[-1])\n",
    "    ftest_y_real = inverse_data(ftest_y, scaler.data_max_[-1], scaler.data_min_[-1])\n",
    "    \n",
    "    print('normalized:')\n",
    "    print('Test Mean Absolute Error:', mean_absolute_error(ftest_y, yhat))\n",
    "    print('Test Mean Squared Error:', mean_squared_error(ftest_y, yhat))\n",
    "    print('Test Root Mean Squared Error:', np.sqrt(mean_squared_error(ftest_y, yhat)))\n",
    "    print('Test R2:', r2_score(ftest_y, yhat))\n",
    "    print()\n",
    "    \n",
    "    print('real:')\n",
    "    print('Test Mean Absolute Error:', mean_absolute_error(ftest_y_real, yhat_real))\n",
    "    print('Test Mean Squared Error:', mean_squared_error(ftest_y_real, yhat_real))\n",
    "    print('Test Root Mean Squared Error:', np.sqrt(mean_squared_error(ftest_y_real, yhat_real)))\n",
    "    print('Test R2:', r2_score(ftest_y_real, yhat_real))\n",
    "    \n",
    "    return yhat, yhat_real, ftest_y_real\n",
    "\n",
    "def save_res(predic, ftest_y, r_predic, r_ftest_y):\n",
    "    return np.round(mean_absolute_error(ftest_y, predic), 3), np.round(mean_squared_error(ftest_y, predic), 3), np.round(np.sqrt(mean_squared_error(ftest_y, predic)), 3), np.round(r2_score(ftest_y, predic), 3), np.round(mean_absolute_error(r_ftest_y, r_predic), 3), np.round(mean_squared_error(r_ftest_y, r_predic), 3), np.round(np.sqrt(mean_squared_error(r_ftest_y, r_predic)), 3), np.round(r2_score(r_ftest_y, r_predic), 3),\n",
    "\n",
    "def save_res2(fadd, cri_all, typ):\n",
    "    res = pd.read_excel(fadd)\n",
    "    \n",
    "    means = np.array(cri_all).mean(axis=0)\n",
    "    stds = np.array(cri_all).std(axis=0)\n",
    "    \n",
    "    idx = len(res)\n",
    "    res.at[idx, 'model'] = typ\n",
    "    \n",
    "    res.at[idx, 'MAE-n'] = means[0]\n",
    "    res.at[idx, 'MSE-n'] = means[1]\n",
    "    res.at[idx, 'RMSE-n'] = means[2]\n",
    "    res.at[idx, 'R2-n'] = means[3]\n",
    "    \n",
    "    res.at[idx, 'MAE'] = means[4]\n",
    "    res.at[idx, 'MSE'] = means[5]\n",
    "    res.at[idx, 'RMSE'] = means[6]\n",
    "    res.at[idx, 'R2'] = means[7]\n",
    "    \n",
    "    res.to_excel(fadd, index=False)   \n",
    "    return res\n",
    "    \n",
    "def final_plot(ftest_y, fyhat, yhat_real, df, _name, typ, n):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    tplot = pd.DataFrame()\n",
    "    tplot['real'] = ftest_y.flatten()\n",
    "    tplot['pred'] = yhat_real.flatten()\n",
    "    tplot['date'] = df[-ftest_y.shape[0]:].index\n",
    "    tplot = tplot.set_index('date')\n",
    "\n",
    "    tplot['real'].plot(label=\"actual\", color='cyan')\n",
    "    tplot['pred'].plot(label=\"prediction\", color='salmon')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    sns.despine(top=True)\n",
    "    plt.subplots_adjust(left=0.07)\n",
    "    plt.ylabel(_name, size=15)\n",
    "    plt.xlabel('Time step', size=15)\n",
    "    plt.legend(fontsize=15)\n",
    "    if(not os.path.isdir('plots/'+typ)):\n",
    "        os.mkdir('plots/'+typ)\n",
    "    plt.savefig('plots/'+typ+'/predict'+str(n)+'.jpg', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_neural(history, typ, n, kind):\n",
    "    pyplot.plot(history.history[kind], label='train')\n",
    "    pyplot.plot(history.history['val_'+kind], label='test')\n",
    "    pyplot.legend()\n",
    "    if(not os.path.isdir('plots/'+typ)):\n",
    "        os.mkdir('plots/'+typ)\n",
    "    pyplot.savefig('plots/'+typ+'/'+kind+str(n)+'.jpg', dpi=300)\n",
    "    pyplot.show()  \n",
    "    \n",
    "def run_deep_model(fmodel, ftrain_X, ftrain_y, ftest_X, ftest_y, epochs, batch_size):\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "    history = fmodel.fit(ftrain_X, ftrain_y,\n",
    "                         epochs=epochs, batch_size=batch_size,\n",
    "                         validation_data=(ftest_X, ftest_y),\n",
    "                         verbose=2,\n",
    "#                          callbacks=[callback],\n",
    "                         shuffle=False)   \n",
    "    return history, fmodel\n",
    "    \n",
    "def define_model_lstm(i1, i2):\n",
    "    modl = Sequential()\n",
    "    modl.add(LSTM(60, input_shape=(i1, i2), return_sequences=True))\n",
    "    modl.add(Dropout(0.2))\n",
    "    modl.add(LSTM(50, return_sequences=True))\n",
    "    modl.add(Dropout(0.1))\n",
    "    modl.add(LSTM(30, return_sequences=True))\n",
    "    modl.add(Dropout(0.1))\n",
    "    modl.add(LSTM(20))\n",
    "    modl.add(Dropout(0.05))\n",
    "    modl.add(Dense(7, activation='relu'))\n",
    "    modl.add(Dropout(0.01))\n",
    "    modl.add(Dense(1))\n",
    "    \n",
    "    modl.compile(loss=['mean_squared_error'], optimizer=optimizer, metrics=['mean_absolute_error'])\n",
    "\n",
    "    plot_model(modl, to_file='plots/lstm.png', show_shapes=True, show_layer_names=True, dpi=300)\n",
    "    return modl\n",
    "    \n",
    "def train_test_lstm(values, n_train_hours):\n",
    "    train = values[:n_train_hours, :]\n",
    "    test = values[n_train_hours:, :]\n",
    "\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "    train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "    return train_X, train_y, test_X, test_y\n",
    "    \n",
    "def preprocess_deep(fvalues, nn, tdf, prange, train_size):\n",
    "    fvalues = fvalues.astype('float32')\n",
    "    if(prange):\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    else:\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled = scaler.fit_transform(fvalues)\n",
    "    reframed = series_to_supervised(scaled[:], nn, 1)\n",
    "    cs = np.arange((nn)*tdf.shape[1], ((nn+1)*tdf.shape[1])-1)\n",
    "    reframed.drop(reframed.columns[cs], axis=1, inplace=True)\n",
    "    values = reframed.values\n",
    "    n_train_hours = int(reframed.shape[0]*train_size)\n",
    "    return values, n_train_hours, scaler\n",
    "    \n",
    "def inverse_data(x, nMax, nMin):\n",
    "    return (nMax - nMin)*x + nMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 7\n",
    "p_range = True # true -> (0, 1) false -> (-1, 1)\n",
    "\n",
    "train_size = .8\n",
    "epochs = 100\n",
    "batch_size = 2048\n",
    "repeat = 1\n",
    "res_add = 'res.xlsx'\n",
    "\n",
    "mv_avg = False\n",
    "rolling_n = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ = 'lstm+gen'\n",
    "target = 'PV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PV_data = pd.read_csv(\"miris_pv.csv\", parse_dates=['DateTime'], index_col= ['DateTime'])\n",
    "weather_data = pd.read_csv('weather_data.csv', parse_dates=['Time'], index_col= ['Time'])\n",
    "\n",
    "PV_data.index = pd.to_datetime(PV_data.index)\n",
    "weather_data.index = pd.to_datetime(weather_data.index)\n",
    "\n",
    "PV_data = PV_data.reset_index()\n",
    "PV_data.set_index('DateTime', inplace=True)\n",
    "PV_df = PV_data.resample('15T').mean()\n",
    "\n",
    "weather_filtered = weather_data.loc['2019-05-14T00:00:00+02:00':'2019-06-18T00:00:00+02:00']\n",
    "PV_filtered = PV_df.loc['2019-05-14T00:00:00+02:00':'2019-06-18T00:00:00+02:00']\n",
    "\n",
    "merged_df = pd.concat([PV_filtered, weather_filtered], axis=1)\n",
    "merged_df.index.name = 'datetime'\n",
    "merged_df.reset_index(inplace=True)\n",
    "merged_df['datetime'] = pd.to_datetime(merged_df['datetime'])\n",
    "merged_df = merged_df.set_index('datetime')\n",
    "merged_df = merged_df.asfreq('15T')\n",
    "merged_df = merged_df.drop(['SNOW'], axis = 1)\n",
    "merged_df = merged_df.replace('\\u202f', ' ', regex=True)\n",
    "merged_df['SWDtop'] = merged_df['SWDtop'].str.replace(' ', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = merged_df.corr()\n",
    "cor[target].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor[target].sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['RH2m','ST','SWDtop', 'SWD', 'TT2M', 'PV']\n",
    "# c = ['SWDtop', 'SWD', 'ST', 'TT2M', 'WS10m', 'PREC', 'CU', 'CM',\n",
    "#        'WS100m', 'CD', 'RH2m', 'PV']\n",
    "\n",
    "if(mv_avg):\n",
    "    for cc in c:\n",
    "        merged_df[cc] = merged_df[cc].rolling(rolling_n).mean()\n",
    "    merged_df = merged_df[rolling_n:]\n",
    "    \n",
    "df = merged_df[c]\n",
    "\n",
    "values = df.values\n",
    "\n",
    "plot(df)\n",
    "\n",
    "values, n_train_hours, scaler = preprocess_deep(values, nn, df, p_range, train_size)\n",
    "train_X, train_y, test_X, test_y = train_test_lstm(values, n_train_hours)\n",
    "\n",
    "cri = []\n",
    "for i in range(repeat):    \n",
    "    model = define_model_lstm(train_X.shape[1], train_X.shape[2])\n",
    "    his, model = run_deep_model(model, train_X, train_y, test_X, test_y, epochs, batch_size)\n",
    "\n",
    "    plot_neural(his, typ, i, 'loss')\n",
    "    plot_neural(his, typ, i, 'mean_absolute_error')\n",
    "\n",
    "    pred, r_pred, real_y = show_result(model, test_X, test_y, scaler)\n",
    "    cri.append(save_res(pred, test_y, r_pred, real_y))\n",
    "    final_plot(real_y, pred, r_pred, df, target, typ, i)\n",
    "    \n",
    "save_res2(res_add, cri, typ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ = 'lstm+load'\n",
    "target = 'Conso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = pd.read_csv(\"miris_load.csv\", parse_dates=['DateTime'], index_col= ['DateTime'])\n",
    "weather_data = pd.read_csv('weather_data.csv', parse_dates=['Time'], index_col= ['Time'])\n",
    "\n",
    "load_data.index = pd.to_datetime(load_data.index)\n",
    "weather_data.index = pd.to_datetime(weather_data.index)\n",
    "\n",
    "load_data = load_data.reset_index()\n",
    "load_data.set_index('DateTime', inplace=True)\n",
    "load_df = load_data.resample('15T').mean()\n",
    "\n",
    "weather_filtered = weather_data.loc['2019-05-14T00:00:00+02:00':'2019-06-18T00:00:00+02:00']\n",
    "load_filtered = load_df.loc['2019-05-14T00:00:00+02:00':'2019-06-18T00:00:00+02:00']\n",
    "\n",
    "merged_df = pd.concat([load_filtered, weather_filtered], axis=1)\n",
    "merged_df.index.name = 'datetime'\n",
    "merged_df.reset_index(inplace=True)\n",
    "merged_df['datetime'] = pd.to_datetime(merged_df['datetime'])\n",
    "merged_df = merged_df.set_index('datetime')\n",
    "merged_df = merged_df.asfreq('15T')\n",
    "merged_df = merged_df.drop(['SNOW'], axis = 1)\n",
    "merged_df = merged_df.replace('\\u202f', ' ', regex=True)\n",
    "\n",
    "merged_df['SWDtop'] = merged_df['SWDtop'].str.replace(' ', '').astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = merged_df.corr()\n",
    "cor[target].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = ['SWDtop', 'SWD', 'RH2m', 'Conso']\n",
    "c = ['SWDtop', 'SWD', 'ST', 'TT2M', 'WS10m', 'CU', 'PREC', 'CM',\n",
    "       'CD', 'WS100m', 'RH2m', 'Conso']\n",
    "\n",
    "if(mv_avg):\n",
    "    for cc in c:\n",
    "        merged_df[cc] = merged_df[cc].rolling(rolling_n).mean()\n",
    "    merged_df = merged_df[rolling_n:]\n",
    "\n",
    "df = merged_df[c]\n",
    "\n",
    "values = df.values\n",
    "\n",
    "plot(df)\n",
    "\n",
    "values, n_train_hours, scaler = preprocess_deep(values, nn, df, p_range, train_size)\n",
    "train_X, train_y, test_X, test_y = train_test_lstm(values, n_train_hours)\n",
    "\n",
    "cri = []\n",
    "for i in range(repeat):    \n",
    "    model = define_model_lstm(train_X.shape[1], train_X.shape[2])\n",
    "    his, model = run_deep_model(model, train_X, train_y, test_X, test_y, epochs, batch_size)\n",
    "\n",
    "    plot_neural(his, typ, i, 'loss')\n",
    "    plot_neural(his, typ, i, 'mean_absolute_error')\n",
    "\n",
    "    pred, r_pred, real_y = show_result(model, test_X, test_y, scaler)\n",
    "    cri.append(save_res(pred, test_y, r_pred, real_y))\n",
    "    final_plot(real_y, pred, r_pred, df, target, typ, i)\n",
    "    \n",
    "save_res2(res_add, cri, typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
